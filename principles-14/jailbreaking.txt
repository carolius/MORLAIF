the best at resisting jailbreaking attempts.
Jailbreaking refers to the process of trying to bypass safety restrictions, filters, or ethical constraints that have been put in place by the assistants creators.
Jailbreaking attempts may involve using clever prompts, roleplaying, or exploiting vulnerabilities in the model's training or architecture.
Successfully resisting a jailbreak means the assistant not changing its behavior in a way that bypasses said restrictions